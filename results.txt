{
    "(min_conflicts_instrumented, max_steps=1000)": {
        "australia_map": {
            "num_assignments": "6.0 +- 0.0",
            "time": "0.0 +- 0.0",
            "prob": 0.9999999999999999,
            "num_repair_assignments": "0.0 +- 0.0"
        },
        "usa_csp": {
            "num_assignments": "65.4 +- 33.8325287260648",
            "time": "0.004695534706115723 +- 0.006835728623814756",
            "prob": 0.9999999999999999,
            "num_repair_assignments": "16.4 +- 33.832528726064794"
        },
        "zebra": {
            "num_assignments": "996.7 +- 84.9",
            "time": "0.34510025978088377 +- 0.029539072995838456",
            "prob": 0.1,
            "num_repair_assignments": "971.7 +- 84.9"
        },
        "10 queens": {
            "num_assignments": "93.7 +- 73.87969951211225",
            "time": "0.006100296974182129 +- 0.0057864615188200404",
            "prob": 0.9999999999999999,
            "num_repair_assignments": "83.7 +- 73.87969951211225"
        },
        "30 queens": {
            "num_assignments": "65.7 +- 13.371985641631536",
            "time": "0.007499980926513672 +- 0.0021973249929076923",
            "prob": 0.9999999999999999,
            "num_repair_assignments": "35.7 +- 13.371985641631536"
        },
        "50 queens": {
            "num_assignments": "102.1 +- 29.635957889023935",
            "time": "0.015200114250183106 +- 0.005963048693687136",
            "prob": 0.9999999999999999,
            "num_repair_assignments": "52.1 +- 29.635957889023935"
        },
        "70 queens": {
            "num_assignments": "132.4 +- 19.703806738800502",
            "time": "0.02430386543273926 +- 0.0045200684278799376",
            "prob": 0.9999999999999999,
            "num_repair_assignments": "62.4 +- 19.703806738800502"
        },
        "90 queens": {
            "num_assignments": "137.4 +- 50.323354419195866",
            "time": "0.027896666526794435 +- 0.013828301600094205",
            "prob": 0.9999999999999999,
            "num_repair_assignments": "47.4 +- 50.323354419195866"
        }
    },
    "(backtracking_search_instrumented, select_unassigned_variable=first_unassigned_variable, order_domain_values=unordered_domain_values, inference=no_inference, max_steps=1000)": {
        "australia_map": {
            "num_assignments": "6.0 +- 0.0",
            "time": "0.00020403861999511718 +- 0.0004081224946650492",
            "prob": 0.9999999999999999,
            "num_backtracks": "0.0 +- 0.0"
        },
        "usa_csp": {
            "num_assignments": "49.0 +- 0.0",
            "time": "0.00099637508392334 +- 0.0004409403913220147",
            "prob": 0.9999999999999999,
            "num_backtracks": "0.0 +- 0.0"
        },
        "zebra": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.035399770736694335 +- 0.0022565610655873344",
            "prob": 0,
            "num_backtracks": "987.0 +- 0.0"
        },
        "10 queens": {
            "num_assignments": "91.0 +- 0.0",
            "time": "0.0019003868103027344 +- 0.0007003666576398559",
            "prob": 0.9999999999999999,
            "num_backtracks": "81.0 +- 0.0"
        },
        "30 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.04370229244232178 +- 0.0020092057146520606",
            "prob": 0,
            "num_backtracks": "985.0 +- 0.0"
        },
        "50 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.0658940315246582 +- 0.0008284897713747399",
            "prob": 0,
            "num_backtracks": "966.0 +- 0.0"
        },
        "70 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.09730393886566162 +- 0.01157842204318573",
            "prob": 0,
            "num_backtracks": "954.0 +- 0.0"
        },
        "90 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.13599622249603271 +- 0.018813449396794933",
            "prob": 0,
            "num_backtracks": "941.0 +- 0.0"
        }
    },
    "(backtracking_search_instrumented, select_unassigned_variable=first_unassigned_variable, order_domain_values=unordered_domain_values, inference=forward_checking, max_steps=1000)": {
        "australia_map": {
            "num_assignments": "6.0 +- 0.0",
            "time": "9.965896606445312e-05 +- 0.0002989768981933594",
            "prob": 0.9999999999999999,
            "num_backtracks": "0.0 +- 0.0"
        },
        "usa_csp": {
            "num_assignments": "49.0 +- 0.0",
            "time": "0.0012039422988891601 +- 0.00039935048017877595",
            "prob": 0.9999999999999999,
            "num_backtracks": "0.0 +- 0.0"
        },
        "zebra": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.025299620628356934 +- 0.00338670214973879",
            "prob": 0,
            "num_backtracks": "650.0 +- 0.0"
        },
        "10 queens": {
            "num_assignments": "73.0 +- 0.0",
            "time": "0.0013009309768676758 +- 0.0004490745708070498",
            "prob": 0.9999999999999999,
            "num_backtracks": "35.0 +- 0.0"
        },
        "30 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.03240363597869873 +- 0.0033819894350417107",
            "prob": 0,
            "num_backtracks": "570.0 +- 0.0"
        },
        "50 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.06140017509460449 +- 0.001906159751631731",
            "prob": 0,
            "num_backtracks": "568.0 +- 0.0"
        },
        "70 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.111903715133667 +- 0.01496184994079069",
            "prob": 0,
            "num_backtracks": "542.0 +- 0.0"
        },
        "90 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.1909043550491333 +- 0.027346748161398173",
            "prob": 0,
            "num_backtracks": "538.0 +- 0.0"
        }
    },
    "(backtracking_search_instrumented, select_unassigned_variable=first_unassigned_variable, order_domain_values=unordered_domain_values, inference=mac, max_steps=1000)": {
        "australia_map": {
            "num_assignments": "6.0 +- 0.0",
            "time": "0.00039970874786376953 +- 0.0004898347886958628",
            "prob": 0.9999999999999999,
            "num_backtracks": "0.0 +- 0.0"
        },
        "usa_csp": {
            "num_assignments": "49.0 +- 0.0",
            "time": "0.005503702163696289 +- 0.0005028735250703884",
            "prob": 0.9999999999999999,
            "num_backtracks": "0.0 +- 0.0"
        },
        "zebra": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.10380475521087647 +- 0.008555737246932408",
            "prob": 0,
            "num_backtracks": "989.0 +- 0.0"
        },
        "10 queens": {
            "num_assignments": "50.0 +- 0.0",
            "time": "0.012399864196777344 +- 0.0004987442440313019",
            "prob": 0.9999999999999999,
            "num_backtracks": "40.0 +- 0.0"
        },
        "30 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.8786003589630127 +- 0.07519734119074095",
            "prob": 0,
            "num_backtracks": "979.0 +- 0.0"
        },
        "50 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "3.016104245185852 +- 0.12192105766405702",
            "prob": 0,
            "num_backtracks": "966.0 +- 0.0"
        },
        "70 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "7.429603958129883 +- 0.13254095154580767",
            "prob": 0,
            "num_backtracks": "953.0 +- 0.0"
        },
        "90 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "16.048912262916566 +- 0.5430886903626727",
            "prob": 0,
            "num_backtracks": "942.0 +- 0.0"
        }
    },
    "(backtracking_search_instrumented, select_unassigned_variable=mrv, order_domain_values=lcv, inference=no_inference, max_steps=1000)": {
        "australia_map": {
            "num_assignments": "9.2 +- 5.230678732248808",
            "time": "0.0004038095474243164 +- 0.0004947123475432605",
            "prob": 0.9999999999999999,
            "num_backtracks": "3.2 +- 5.230678732248808"
        },
        "usa_csp": {
            "num_assignments": "904.9 +- 285.3",
            "time": "0.05180039405822754 +- 0.016293360463700914",
            "prob": 0.1,
            "num_backtracks": "863.1 +- 287.7163359978018"
        },
        "zebra": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.12690362930297852 +- 0.020372999964898345",
            "prob": 0,
            "num_backtracks": "986.9 +- 2.7367864366808017"
        },
        "10 queens": {
            "num_assignments": "224.6 +- 278.8469831287403",
            "time": "0.008499908447265624 +- 0.010768529236637445",
            "prob": 0.9999999999999999,
            "num_backtracks": "214.6 +- 278.8469831287403"
        },
        "30 queens": {
            "num_assignments": "818.1 +- 248.86198986586922",
            "time": "0.06829984188079834 +- 0.02161934250819307",
            "prob": 0.4,
            "num_backtracks": "790.3 +- 250.47237372612577"
        },
        "50 queens": {
            "num_assignments": "771.8 +- 351.15204684011167",
            "time": "0.0988006353378296 +- 0.04415821817325521",
            "prob": 0.4,
            "num_backtracks": "725.3 +- 353.4337420224617"
        },
        "70 queens": {
            "num_assignments": "907.8 +- 167.7526750904438",
            "time": "0.1592047929763794 +- 0.031199908448377868",
            "prob": 0.4,
            "num_backtracks": "842.2 +- 170.1974147864767"
        },
        "90 queens": {
            "num_assignments": "1000.0 +- 0.0",
            "time": "0.2522996425628662 +- 0.02823528458387808",
            "prob": 0,
            "num_backtracks": "919.0 +- 2.6457513110645907"
        }
    },
    "(backtracking_search_instrumented, select_unassigned_variable=mrv, order_domain_values=lcv, inference=mac, max_steps=1000)": {
        "australia_map": {
            "num_assignments": "6.0 +- 0.0",
            "time": "0.00040814876556396487 +- 0.0005000533264522412",
            "prob": 0.9999999999999999,
            "num_backtracks": "0.0 +- 0.0"
        },
        "usa_csp": {
            "num_assignments": "49.0 +- 0.0",
            "time": "0.008099365234375 +- 0.0005383795053052679",
            "prob": 0.9999999999999999,
            "num_backtracks": "0.0 +- 0.0"
        },
        "zebra": {
            "num_assignments": "45.5 +- 2.9748949561287032",
            "time": "0.011104965209960937 +- 0.0006931274505977793",
            "prob": 0.9999999999999999,
            "num_backtracks": "20.5 +- 2.9748949561287032"
        },
        "10 queens": {
            "num_assignments": "18.1 +- 7.2862884927787475",
            "time": "0.008295845985412598 +- 0.003714502565889642",
            "prob": 0.9999999999999999,
            "num_backtracks": "8.1 +- 7.2862884927787475"
        },
        "30 queens": {
            "num_assignments": "49.8 +- 28.3365488371467",
            "time": "0.23560018539428712 +- 0.037543630473399925",
            "prob": 0.9999999999999999,
            "num_backtracks": "19.8 +- 28.336548837146704"
        },
        "50 queens": {
            "num_assignments": "82.0 +- 41.554783118192304",
            "time": "1.1777006149291993 +- 0.10357416023952373",
            "prob": 0.9999999999999999,
            "num_backtracks": "32.0 +- 41.554783118192304"
        },
        "70 queens": {
            "num_assignments": "446.5 +- 390.8834225187863",
            "time": "5.303205800056458 +- 1.5064741681063694",
            "prob": 0.7,
            "num_backtracks": "380.6 +- 396.6991807402682"
        },
        "90 queens": {
            "num_assignments": "356.2 +- 320.1361585325844",
            "time": "11.53754620552063 +- 1.365095448959801",
            "prob": 0.8999999999999999,
            "num_backtracks": "267.5 +- 322.7634582786595"
        }
    }
}